{
	"metadata": {
		"kernelspec": {
			"display_name": "Glue PySpark",
			"language": "python",
			"name": "glue_pyspark"
		},
		"language_info": {
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"file_extension": ".py",
			"mimetype": "text/x-python",
			"name": "Python_Glue_Session",
			"pygments_lexer": "python3"
		},
		"toc-autonumbering": false,
		"toc-showcode": true,
		"toc-showmarkdowntxt": false,
		"toc-showtags": true
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "markdown",
			"source": "# ANT345 - Multicloud Analytics - Glue Notebook\n##### We are now running an AWS Glue Studio notebook, and can use Glue Interactive Sessions to initiate a Spark shell\n",
			"metadata": {
				"editable": true,
				"tags": [],
				"trusted": true
			}
		},
		{
			"cell_type": "markdown",
			"source": "#### Use the `%help` command to see available notebook commands (\"magics\").\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "%help",
			"metadata": {
				"trusted": true,
				"tags": [],
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"execution_count": 2,
			"outputs": [
				{
					"name": "stdout",
					"text": "Welcome to the Glue Interactive Sessions Kernel\nFor more information on available magic commands, please type %help in any new cell.\n\nPlease view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\nInstalled kernel version: 1.0.2 \n",
					"output_type": "stream"
				},
				{
					"output_type": "display_data",
					"data": {
						"text/markdown": "\n# Available Magic Commands\n\n## Sessions Magic\n\n----\n    %help                             Return a list of descriptions and input types for all magic commands. \n    %profile            String        Specify a profile in your aws configuration to use as the credentials provider.\n    %region             String        Specify the AWS region in which to initialize a session. \n                                      Default from ~/.aws/config on Linux or macOS, \n                                      or C:\\Users\\ USERNAME \\.aws\\config\" on Windows.\n    %idle_timeout       Int           The number of minutes of inactivity after which a session will timeout. \n                                      Default: 2880 minutes (48 hours).\n    %session_id_prefix  String        Define a String that will precede all session IDs in the format \n                                      [session_id_prefix]-[session_id]. If a session ID is not provided,\n                                      a random UUID will be generated.\n    %status                           Returns the status of the current Glue session including its duration, \n                                      configuration and executing user / role.\n    %session_id                       Returns the session ID for the running session. \n    %list_sessions                    Lists all currently running sessions by ID.\n    %stop_session                     Stops the current session.\n    %glue_version       String        The version of Glue to be used by this session. \n                                      Currently, the only valid options are 2.0, 3.0 and 4.0. \n                                      Default: 2.0.\n----\n\n## Selecting Session Types\n\n----\n    %streaming          String        Sets the session type to Glue Streaming.\n    %etl                String        Sets the session type to Glue ETL.\n    %glue_ray           String        Sets the session type to Glue Ray.\n    %session_type       String        Specify a session_type to be used. Supported values: streaming, etl and glue_ray. \n----\n\n## Glue Config Magic \n*(common across all session types)*\n\n----\n\n    %%configure         Dictionary    A json-formatted dictionary consisting of all configuration parameters for \n                                      a session. Each parameter can be specified here or through individual magics.\n    %iam_role           String        Specify an IAM role ARN to execute your session with.\n                                      Default from ~/.aws/config on Linux or macOS, \n                                      or C:\\Users\\%USERNAME%\\.aws\\config` on Windows.\n    %number_of_workers  int           The number of workers of a defined worker_type that are allocated \n                                      when a session runs.\n                                      Default: 5.\n    %additional_python_modules  List  Comma separated list of additional Python modules to include in your cluster \n                                      (can be from Pypi or S3).\n    %%tags        Dictionary          Specify a json-formatted dictionary consisting of tags to use in the session.\n    \n    %%assume_role Dictionary, String  Specify a json-formatted dictionary or an IAM role ARN string to create a session \n                                      for cross account access.\n                                      E.g. {valid arn}\n                                      %%assume_role \n                                      'arn:aws:iam::XXXXXXXXXXXX:role/AWSGlueServiceRole' \n                                      E.g. {credentials}\n                                      %%assume_role\n                                      {\n                                            \"aws_access_key_id\" : \"XXXXXXXXXXXX\",\n                                            \"aws_secret_access_key\" : \"XXXXXXXXXXXX\",\n                                            \"aws_session_token\" : \"XXXXXXXXXXXX\"\n                                       }\n----\n\n                                      \n## Magic for Spark Sessions (ETL & Streaming)\n\n----\n    %worker_type        String        Set the type of instances the session will use as workers. \n    %connections        List          Specify a comma separated list of connections to use in the session.\n    %extra_py_files     List          Comma separated list of additional Python files From S3.\n    %extra_jars         List          Comma separated list of additional Jars to include in the cluster.\n    %spark_conf         String        Specify custom spark configurations for your session. \n                                      E.g. %spark_conf spark.serializer=org.apache.spark.serializer.KryoSerializer\n----\n                                      \n## Magic for Ray Session\n\n----\n    %min_workers        Int           The minimum number of workers that are allocated to a Ray session. \n                                      Default: 1.\n    %object_memory_head Int           The percentage of free memory on the instance head node after a warm start. \n                                      Minimum: 0. Maximum: 100.\n    %object_memory_worker Int         The percentage of free memory on the instance worker nodes after a warm start. \n                                      Minimum: 0. Maximum: 100.\n----\n\n## Action Magic\n\n----\n\n    %%sql               String        Run SQL code. All lines after the initial %%sql magic will be passed\n                                      as part of the SQL code.  \n----\n\n"
					},
					"metadata": {}
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "####  Use magics to set up our Interactive Session settings (idle timeout, version of Glue, etc).\n#### We can also specify our multi-cloud connections in this section. \n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "%idle_timeout 80\n%glue_version 4.0\n%worker_type G.1X\n%number_of_workers 5\n%connections bigquery,snowflake,ADLSgen2",
			"metadata": {
				"trusted": true,
				"editable": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"execution_count": 8,
			"outputs": [
				{
					"name": "stdout",
					"text": "Current idle_timeout is None minutes.\nidle_timeout has been set to 80 minutes.\nSetting Glue version to: 4.0\nPrevious worker type: None\nSetting new worker type to: G.1X\nPrevious number of workers: None\nSetting new number of workers to: 5\nConnections to be included:\nbigquery\nsnowflake\nADLSgen2\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "#### Import relevant libraries for our job, and initiate the Spark/Glue context and Spark session",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "import sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\nfrom awsglue.dynamicframe import DynamicFrame\nfrom pyspark.sql.functions import format_number\nimport pyspark.sql.functions as sf\n  \nsc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)",
			"metadata": {
				"trusted": true,
				"tags": [],
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"execution_count": 1,
			"outputs": [
				{
					"name": "stdout",
					"text": "Trying to create a Glue session for the kernel.\nSession Type: glueetl\nWorker Type: G.1X\nNumber of Workers: 5\nSession ID: ce03f012-54cf-4c8b-856e-00a8caa234d6\nApplying the following default arguments:\n--glue_kernel_version 1.0.2\n--enable-glue-datacatalog true\nWaiting for session ce03f012-54cf-4c8b-856e-00a8caa234d6 to get into ready status...\nSession ce03f012-54cf-4c8b-856e-00a8caa234d6 has been created.\n\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "#### Create Glue DynamicFrame using the **bigquery** connection. \nOptions that need to be included when running a query using this connector include:\n- materializationDataset: This specifies a BigQuery dataset that can be used to store a temporary table containing the result of the query we run\n- parentProject: This specifies the name of the GCP parent project that contains the BigQuery resource we want to query\n- viewsEnabled: This option must be set to true in order to run a SQL query using the connector\n- query: This specifies the SQL query that we want to run\n- connectionName: The name of the AWS Glue connection we want to use\n\nNote that the SQL query we specify here is executed by BigQuery, and only the results of the query are transferred over the network from GCP to AWS. The query we specify aggregates the raw daily data we have in BigQuery (covering each day over the last 2.5 years) and calculates the net sales by year and month. This is much more efficient than loading all the data from the table in GCP, and then filtering or aggregating the data in Spark.",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "bq_web_net_sales = glueContext.create_dynamic_frame.from_options(\n    connection_type=\"bigquery\",\n    connection_options={\n        \"materializationDataset\": \"TempViews\",\n        \"parentProject\": \"multicloud-analytics\",\n        \"viewsEnabled\": \"true\",\n        \"query\": \"\"\"\n        WITH web_sales_date AS (\n        select *,\n            round(round(web_sales, 2) - round(web_returns, 2), 2) as net_sales,\n            extract(month from cast(date as timestamp)) date_month,\n            extract(year from cast(date as timestamp)) date_year\n        from `multicloud-analytics.AnyCompany.daily_web_sales` \n        )\n\n        select\n            date_year, date_month, round(SUM(net_sales),2) as web_net_sales\n        from web_sales_date\n        group by date_year, date_month\n        order by date_year, date_month asc\n        \"\"\",\n        \"connectionName\": \"bigquery\",\n    },\n)\n        ",
			"metadata": {
				"trusted": true,
				"tags": [],
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"execution_count": 2,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Print bq_net_web_sales schema\nbq_web_net_sales.printSchema()\n\n# Display contents of bq_web_net_sales, sorted by date_year then date_month\nbq_web_net_sales.toDF().sort(\"date_year\", \"date_month\").show()\n\n# Count the rows in bq_web_net_sales\nbq_web_net_sales.count()",
			"metadata": {
				"trusted": true,
				"tags": [],
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"execution_count": 3,
			"outputs": [
				{
					"name": "stdout",
					"text": "root\n|-- date_year: long\n|-- date_month: long\n|-- web_net_sales: double\n\n+---------+----------+-------------+\n|date_year|date_month|web_net_sales|\n+---------+----------+-------------+\n|     2021|         1|    796957.39|\n|     2021|         2|    705379.77|\n|     2021|         3|     778931.1|\n|     2021|         4|    732405.26|\n|     2021|         5|    768200.52|\n|     2021|         6|    744789.88|\n|     2021|         7|    777226.65|\n|     2021|         8|     755394.4|\n|     2021|         9|    729603.49|\n|     2021|        10|    758422.85|\n|     2021|        11|    708773.45|\n|     2021|        12|    759453.98|\n|     2022|         1|    780733.38|\n|     2022|         2|    704074.24|\n|     2022|         3|    785959.02|\n|     2022|         4|    719178.31|\n|     2022|         5|    748272.72|\n|     2022|         6|    778954.65|\n|     2022|         7|    754346.96|\n|     2022|         8|    710306.32|\n+---------+----------+-------------+\nonly showing top 20 rows\n\n33\n/opt/amazon/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:127: UserWarning: DataFrame constructor is internal. Do not directly use it.\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "#### Create Glue DynamicFrame using the **snowflake** connection. \nOptions that need to be included when running a query using this connector include:\n- autopushdown: This option must be set to *on* in order to specify a query that you want Snowflake to run\n- connectionName: The name of the AWS Glue connection we want to use\n- sfDatabase: The name of the Snowflake database containing the table you want to query\n- query: This specifies the SQL query that you want to run\n\nThe query we specify is run by Snowflake, and aggregates the raw daily sales data for our stores to calculate net sales, by year nad month. Only the results of the query are passed over the network from Snowflake to Glue. ",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "sf_store_net_sales = glueContext.create_dynamic_frame.from_options(\n    connection_type=\"snowflake\",\n    connection_options={\n        \"autopushdown\": \"on\",\n        \"connectionName\": \"snowflake\",\n        \"sfDatabase\": \"ANYCOMPANY\",\n        \"query\": \"\"\"\n        WITH store_sales_date AS (\n                select *,\n                    round(round(store_sales, 2) - round(store_returns, 2), 2) as net_sales,\n                    extract(month from sales_date) date_month,\n                    extract(year from sales_date) date_year\n                from store_sales\n            )\n\n            select date_year, date_month, round(SUM(net_sales),2) as store_net_sales\n            from store_sales_date\n            group by date_year, date_month\n            order by date_year, date_month asc\n            \"\"\",\n    }\n)",
			"metadata": {
				"trusted": true,
				"tags": [],
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"execution_count": 4,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Print sf_store_net_sales schema\nsf_store_net_sales.printSchema()\n\n# Display contents of sf_store_net_sales, sorted by date_year then date_month\nsf_store_net_sales.toDF().sort(\"date_year\", \"date_month\").show(20)\n\n# Count the rows in sf_store_net_sales\nsf_store_net_sales.count()",
			"metadata": {
				"trusted": true,
				"tags": [],
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"execution_count": 5,
			"outputs": [
				{
					"name": "stdout",
					"text": "root\n|-- DATE_YEAR: decimal\n|-- DATE_MONTH: decimal\n|-- STORE_NET_SALES: decimal\n\n+---------+----------+---------------+\n|DATE_YEAR|DATE_MONTH|STORE_NET_SALES|\n+---------+----------+---------------+\n|     2021|         1|      995270.22|\n|     2021|         2|      883355.84|\n|     2021|         3|      975316.06|\n|     2021|         4|      951041.91|\n|     2021|         5|      964885.17|\n|     2021|         6|      917237.75|\n|     2021|         7|     1012048.65|\n|     2021|         8|      972043.88|\n|     2021|         9|      934952.16|\n|     2021|        10|      996833.78|\n|     2021|        11|      934065.69|\n|     2021|        12|      961486.12|\n|     2022|         1|      986548.55|\n|     2022|         2|      852694.85|\n|     2022|         3|      986495.57|\n|     2022|         4|      957231.21|\n|     2022|         5|      974230.58|\n|     2022|         6|      957339.40|\n|     2022|         7|      980734.73|\n|     2022|         8|      961008.91|\n+---------+----------+---------------+\nonly showing top 20 rows\n\n33\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "#### Join the BigQuery and Snowflake aggregated table data to create a new *total_net_sales* DynamicFrame\nIn this code block we do the following:\n- We join the two DynamicFrames on the year and month columns. Note that Snowflake column names are in uppercase\n- We drop the unneeded data and month columns\n- We print the DynamicFrame schema\n- We show a sample of content from the DynamicFrame, but convert it to a Spark dataframe so we can use the sort function\n- We count the rows in the DynamicFrame",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "total_net_sales = Join.apply(bq_web_net_sales, sf_store_net_sales, [\"date_year\",\"date_month\"], [\"DATE_YEAR\",\"DATE_MONTH\"])\n# Delete unneeded columns\ntotal_net_sales = total_net_sales.drop_fields([\"DATE_YEAR\",\"DATE_MONTH\"])\n# Print Schema, then show contents of total_net_sales, and count rows\ntotal_net_sales.printSchema()\ntotal_net_sales.toDF().sort(\"date_year\", \"date_month\").show(20)\ntotal_net_sales.count()",
			"metadata": {
				"trusted": true,
				"tags": [],
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"execution_count": 6,
			"outputs": [
				{
					"name": "stdout",
					"text": "root\n|-- date_month: long\n|-- STORE_NET_SALES: decimal\n|-- date_year: long\n|-- web_net_sales: double\n\n+----------+---------------+---------+-------------+\n|date_month|STORE_NET_SALES|date_year|web_net_sales|\n+----------+---------------+---------+-------------+\n|         1|      995270.22|     2021|    796957.39|\n|         2|      883355.84|     2021|    705379.77|\n|         3|      975316.06|     2021|     778931.1|\n|         4|      951041.91|     2021|    732405.26|\n|         5|      964885.17|     2021|    768200.52|\n|         6|      917237.75|     2021|    744789.88|\n|         7|     1012048.65|     2021|    777226.65|\n|         8|      972043.88|     2021|     755394.4|\n|         9|      934952.16|     2021|    729603.49|\n|        10|      996833.78|     2021|    758422.85|\n|        11|      934065.69|     2021|    708773.45|\n|        12|      961486.12|     2021|    759453.98|\n|         1|      986548.55|     2022|    780733.38|\n|         2|      852694.85|     2022|    704074.24|\n|         3|      986495.57|     2022|    785959.02|\n|         4|      957231.21|     2022|    719178.31|\n|         5|      974230.58|     2022|    748272.72|\n|         6|      957339.40|     2022|    778954.65|\n|         7|      980734.73|     2022|    754346.96|\n|         8|      961008.91|     2022|    710306.32|\n+----------+---------------+---------+-------------+\nonly showing top 20 rows\n\n33\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "#### Create Glue DynamicFrame using the **Azure Data Lake Storage (ADLS)** connection. \nOptions that need to be included when running a query using this connector include:\n- fileFormat: Specifies the format of the file stored in ADLS (in this case, a CSV file)\n- path: The path in ADLS for the file we want to load data from\n- header: When working with a CSV file the header option specifies if the CSV file has a header line with column names\n- connectionName: The name of the AWS Glue connection\n\nNote that with the ADLS connector all data is read from the specified file - you are not able to specify a query to run. In our case, we are reading a limited amount of data that contains the Consumer Confidence Index score for the past 2.5 years. ",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "ADLS_consumer_confidence = glueContext.create_dynamic_frame.from_options(\n        connection_type=\"marketplace.spark\",\n        connection_options={\n            \"fileFormat\": \"csv\",\n            \"path\": \"/consumer_confidence.csv\",\n            \"header\": \"true\",\n            \"connectionName\": \"ADLSgen2\",\n        },\n        transformation_ctx=\"ADLS_consumer_confidence\",\n    )\n\nADLS_consumer_confidence.toDF().show()",
			"metadata": {
				"trusted": true,
				"vscode": {
					"languageId": "plaintext"
				}
			},
			"execution_count": 7,
			"outputs": [
				{
					"name": "stdout",
					"text": "+----+-----+---------+\n|year|month|cci_value|\n+----+-----+---------+\n|2023|    7|      117|\n|2022|   12|    108.3|\n|2023|    4|    101.3|\n|2021|   11|    111.9|\n|2023|    2|    103.4|\n|2021|    9|    109.8|\n|2021|    4|    117.5|\n|2021|    1|     88.9|\n|2022|    9|    107.8|\n|2022|   11|    101.4|\n|2021|   12|    115.2|\n|2023|    9|      103|\n|2021|    6|    128.9|\n|2022|    2|    105.7|\n|2021|    3|      109|\n|2022|    5|    103.2|\n|2022|    4|    108.6|\n|2022|   10|    102.5|\n|2022|    7|     95.3|\n|2022|    3|    107.6|\n+----+-----+---------+\nonly showing top 20 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "#### Join the *total_net_sales* and *ADLS_consumer_confidence* DynamicFrames\nIn this code block we do the following:\n- Create a new DynamicFrame called total_net_sales_with_confidence by joining the previously create total_net_sales DynamicFrame with the consumer confidence index data retrieved from Azure Data Lake Storage. \n- Delete uneeded columns (year and month)\n- Display a sample of the data from the new DynamicFrame (although we convert to a DataFrame for improved formatting display)",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "# Join total_net_sales and ADLS_consumer_confidence\ntotal_net_sales_with_confidence = Join.apply(total_net_sales, ADLS_consumer_confidence, [\"date_year\",\"date_month\"], [\"year\",\"month\"])\n# Delete unneeded columns\ntotal_net_sales_with_confidence = total_net_sales_with_confidence.drop_fields([\"year\",\"month\"])\n# Display contents of total_net_sales_with_confidence\ntotal_net_sales_with_confidence.toDF().show()",
			"metadata": {
				"trusted": true,
				"tags": [],
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"execution_count": 8,
			"outputs": [
				{
					"name": "stdout",
					"text": "+----------+---------------+---------+-------------+---------+\n|date_month|STORE_NET_SALES|cci_value|web_net_sales|date_year|\n+----------+---------------+---------+-------------+---------+\n|         2|      883355.84|     90.4|    705379.77|     2021|\n|         2|      852694.85|    105.7|    704074.24|     2022|\n|         1|      995270.22|     88.9|    796957.39|     2021|\n|        12|      961486.12|    115.2|    759453.98|     2021|\n|         3|      975316.06|      109|     778931.1|     2021|\n|         3|      986495.57|    107.6|    785959.02|     2022|\n|         4|      951041.91|    117.5|    732405.26|     2021|\n|         2|      870137.75|    103.4|    704881.79|     2023|\n|        12|      980373.87|    108.3|    767855.13|     2022|\n|        11|      934065.69|    111.9|    708773.45|     2021|\n|         1|      986548.55|    111.1|    780733.38|     2022|\n|        10|      996833.78|    111.6|    758422.85|     2021|\n|         4|      957231.21|    108.6|    719178.31|     2022|\n|         3|      994740.97|      104|    783083.26|     2023|\n|         1|      965816.93|      106|    771938.02|     2023|\n|        11|      950985.33|    101.4|    734400.07|     2022|\n|         5|      964885.17|      120|    768200.52|     2021|\n|        10|      948704.40|    102.5|    783680.93|     2022|\n|         5|      974230.58|    103.2|    748272.72|     2022|\n|         4|      961812.38|    101.3|    727434.95|     2023|\n+----------+---------------+---------+-------------+---------+\nonly showing top 20 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "#### Final formatting\nIn this  code block, we do some final formatting. \n- We rename the STORE_NET_SALES column (which came from Snowflake) to have a lowercase name\n- We change the order of the columns using the DynamicFrame *select_fields* function\n- We display some sample data from the DynamicFrame (but again convert it to a DataFrame for improved formatting)",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "# Rename STORE_NET_SALES to store_net_sales\ntotal_net_sales_with_confidence = total_net_sales_with_confidence.rename_field(\"STORE_NET_SALES\", \"store_net_sales\")\n\n# Change column order\ntotal_net_sales_with_confidence = total_net_sales_with_confidence.select_fields([\"date_year\",\"date_month\",\"store_net_sales\",\"web_net_sales\",\"cci_value\"])\ntotal_net_sales_with_confidence.toDF().sort(\"date_year\", \"date_month\").show()",
			"metadata": {
				"trusted": true,
				"tags": [],
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"execution_count": 9,
			"outputs": [
				{
					"name": "stdout",
					"text": "+---------+----------+---------------+-------------+---------+\n|date_year|date_month|store_net_sales|web_net_sales|cci_value|\n+---------+----------+---------------+-------------+---------+\n|     2021|         1|      995270.22|    796957.39|     88.9|\n|     2021|         2|      883355.84|    705379.77|     90.4|\n|     2021|         3|      975316.06|     778931.1|      109|\n|     2021|         4|      951041.91|    732405.26|    117.5|\n|     2021|         5|      964885.17|    768200.52|      120|\n|     2021|         6|      917237.75|    744789.88|    128.9|\n|     2021|         7|     1012048.65|    777226.65|    125.1|\n|     2021|         8|      972043.88|     755394.4|    115.2|\n|     2021|         9|      934952.16|    729603.49|    109.8|\n|     2021|        10|      996833.78|    758422.85|    111.6|\n|     2021|        11|      934065.69|    708773.45|    111.9|\n|     2021|        12|      961486.12|    759453.98|    115.2|\n|     2022|         1|      986548.55|    780733.38|    111.1|\n|     2022|         2|      852694.85|    704074.24|    105.7|\n|     2022|         3|      986495.57|    785959.02|    107.6|\n|     2022|         4|      957231.21|    719178.31|    108.6|\n|     2022|         5|      974230.58|    748272.72|    103.2|\n|     2022|         6|      957339.40|    778954.65|     98.7|\n|     2022|         7|      980734.73|    754346.96|     95.3|\n|     2022|         8|      961008.91|    710306.32|    103.6|\n+---------+----------+---------------+-------------+---------+\nonly showing top 20 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "#### Write the new table to Amazon S3\nIn this final code block we write the data from our final DynamicFrame to Amazon S3. \n\nThe data file we write to S3 contains data that was joined across 3 non-AWS data sources -- *Google BigQuery*, *Snowflake*, and *Azure Data Lake Storage*. ",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "# Write total_net_sales_with_confidence to S3 and store in Glue catalog\nglueContext.write_dynamic_frame.from_options(\n    frame=total_net_sales_with_confidence,\n    connection_type=\"s3\",\n    format=\"csv\",\n    connection_options={\n        \"path\": \"s3://multicloud-analytics-output/total_net_sales_with_confidence\",\n    },\n)",
			"metadata": {
				"trusted": true,
				"tags": [],
				"vscode": {
					"languageId": "plaintext"
				}
			},
			"execution_count": 10,
			"outputs": [
				{
					"name": "stdout",
					"text": "<awsglue.dynamicframe.DynamicFrame object at 0x7f8bdd309870>\n",
					"output_type": "stream"
				}
			]
		}
	]
}